{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparkify Project Workspace\n",
    "This workspace contains a tiny subset (128MB) of the full dataset available (12GB). Feel free to use this workspace to build your project, or to explore a smaller subset with Spark before deploying your cluster on the cloud. Instructions for setting up your Spark cluster is included in the last lesson of the Extracurricular Spark Course content.\n",
    "\n",
    "You can follow the steps below to guide your data analysis and model building portion of this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#only for my local spark setup on windows\n",
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = 'pyspark-shell'\n",
    "\n",
    "#import libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, lit\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "from pyspark.sql.types import IntegerType, DateType, StructType,StructField, StringType, FloatType\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler, MinMaxScaler, OneHotEncoder\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier, GBTClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "import re\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "import random\n",
    "import datetime\n",
    "from pyspark.sql import DataFrame\n",
    "from ua_parser import user_agent_parser\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ua-parser in c:\\users\\aditya\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages (0.10.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.1.1; however, version 20.2.3 is available.\n",
      "You should consider upgrading via the 'c:\\users\\aditya\\appdata\\local\\programs\\python\\python38-32\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "# install ua-parser for url parsing. Credit: https://github.com/ua-parser/uap-python\n",
    "! pip install ua-parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[8]\") \\\n",
    "    .appName(\"Sparkify Project 1\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Clean Dataset\n",
    "In this workspace, the mini-dataset file is `mini_sparkify_event_data.json`. Load and clean the dataset, checking for invalid or missing data - for example, records without userids or sessionids. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load mini dataset\n",
    "df = spark.read.json('mini_sparkify_event_data.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- artist: string (nullable = true)\n",
      " |-- auth: string (nullable = true)\n",
      " |-- firstName: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- itemInSession: long (nullable = true)\n",
      " |-- lastName: string (nullable = true)\n",
      " |-- length: double (nullable = true)\n",
      " |-- level: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- method: string (nullable = true)\n",
      " |-- page: string (nullable = true)\n",
      " |-- registration: long (nullable = true)\n",
      " |-- sessionId: long (nullable = true)\n",
      " |-- song: string (nullable = true)\n",
      " |-- status: long (nullable = true)\n",
      " |-- ts: long (nullable = true)\n",
      " |-- userAgent: string (nullable = true)\n",
      " |-- userId: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# look at all the columns and datatypes\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "286500"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#no. of records\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems these are records of every interaction a user has had with the app. There are 286,500 records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(artist='Martha Tilston', auth='Logged In', firstName='Colin', gender='M', itemInSession=50, lastName='Freeman', length=277.89016, level='paid', location='Bakersfield, CA', method='PUT', page='NextSong', registration=1538173362000, sessionId=29, song='Rockpools', status=200, ts=1538352117000, userAgent='Mozilla/5.0 (Windows NT 6.1; WOW64; rv:31.0) Gecko/20100101 Firefox/31.0', userId='30')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|status|\n",
      "+------+\n",
      "|   307|\n",
      "|   404|\n",
      "|   200|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"status\").dropDuplicates().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+\n",
      "|page                     |\n",
      "+-------------------------+\n",
      "|Cancel                   |\n",
      "|Submit Downgrade         |\n",
      "|Thumbs Down              |\n",
      "|Home                     |\n",
      "|Downgrade                |\n",
      "|Roll Advert              |\n",
      "|Logout                   |\n",
      "|Save Settings            |\n",
      "|Cancellation Confirmation|\n",
      "|About                    |\n",
      "|Submit Registration      |\n",
      "|Settings                 |\n",
      "|Login                    |\n",
      "|Register                 |\n",
      "|Add to Playlist          |\n",
      "|Add Friend               |\n",
      "|NextSong                 |\n",
      "|Thumbs Up                |\n",
      "|Help                     |\n",
      "|Upgrade                  |\n",
      "|Error                    |\n",
      "|Submit Upgrade           |\n",
      "+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"page\").dropDuplicates().show(30, truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "226"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(\"userId\").dropDuplicates().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My assumptions for the content :  \n",
    "\n",
    "1.) Artist, Song : The artist & song being listened to. Useful for genre classifications, but may not be as usueful as a search event for an artist would be.  \n",
    "\n",
    "2.) Firstname, Lastname, Location, gender, userId : The specifics. Only the userId, gender and location are likely relevant to us.  \n",
    "\n",
    "3.) Auth : This seems directly connected to the userId column, as in you need to be logged in for that to populate. Other items like firstname etc wont be populated unless you are logged in, so we focus on this  \n",
    "\n",
    "4.) Method, status : Status is an unknown variable, with just three numbers which are unclear - likely an event code? Method is the HTTP method, which is not relevant and will just have PUT and GET\n",
    "\n",
    "5.) sessionId : Seems to be generated per session and not unique to user.\n",
    "\n",
    "6.) Registration and TS: TS is simply the timestamp of the event. Registration is likely when they registered. \n",
    "\n",
    "7.) Level : A critical value, free vs paid\n",
    "\n",
    "8.) UserAgent : The broser/device where they accessed the service from.\n",
    "\n",
    "9.) Page : The actual event. Rather interesting, see above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------------+\n",
      "|count(userId)|count(sessionId)|\n",
      "+-------------+----------------+\n",
      "|            0|               0|\n",
      "+-------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Check for Nulls - check in userid's and session id's as they are identifiers. \n",
    "df.createOrReplaceTempView(\"spark_table\")\n",
    "\n",
    "spark.sql('''\n",
    "            SELECT COUNT(userId), COUNT(sessionId) \n",
    "            FROM spark_table \n",
    "            WHERE userId IS NULL OR sessionId IS NULL\n",
    "            '''\n",
    "            ).show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|count(userId)|\n",
      "+-------------+\n",
      "|         8346|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#No nulls. lets check blank user id's\n",
    "\n",
    "spark.sql('''\n",
    "        SELECT COUNT(userId)\n",
    "        FROM spark_table \n",
    "        WHERE userId =''\n",
    "        '''\n",
    "        ).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|count(sessionId)|\n",
      "+----------------+\n",
      "|               0|\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# lets check blank sessionid's\n",
    "\n",
    "spark.sql('''\n",
    "        SELECT COUNT(sessionId)\n",
    "        FROM spark_table \n",
    "        WHERE sessionId =''\n",
    "        '''\n",
    "        ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No blank sessionid's. Makes sense, as any activity would be part of a session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+---------+\n",
      "|userId| page|sessionId|\n",
      "+------+-----+---------+\n",
      "|      | Home|        8|\n",
      "|      | Help|        8|\n",
      "|      | Home|        8|\n",
      "|      |Login|        8|\n",
      "|      | Home|      240|\n",
      "|      |Login|      240|\n",
      "|      |Login|      100|\n",
      "|      |Login|      241|\n",
      "|      | Home|      187|\n",
      "|      |Login|      187|\n",
      "|      | Home|      187|\n",
      "|      | Home|      187|\n",
      "|      |Login|      187|\n",
      "|      | Home|       27|\n",
      "|      |About|       27|\n",
      "|      | Home|       27|\n",
      "|      | Home|      187|\n",
      "|      |Login|      187|\n",
      "|      | Home|      257|\n",
      "|      | Home|      100|\n",
      "+------+-----+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#taking a look at blank user id's\n",
    "\n",
    "spark.sql('''\n",
    "        SELECT userId, page, sessionId\n",
    "        FROM spark_table\n",
    "        WHERE userId =''\n",
    "        '''\n",
    "        ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems the blank id's are associated with non-logged in users, which makes sense. We should drop these, as there is no concept of churn when it comes to unlogged users - we cannot differentiate them. However, they might be useful if we wanted to, say, test some changes which lead to more users signing up.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing blank user id's\n",
    "df_new = df.filter(df['userId']!='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fix the timestamp\n",
    "fix_time = udf(lambda x: datetime.datetime.fromtimestamp(x / 1000.0).strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "df_new = df_new.withColumn(\"time\", fix_time(df_new.ts))\n",
    "\n",
    "#split the location into city and state\n",
    "city_name = udf(lambda x:x.split(\", \")[0])\n",
    "state_name = udf(lambda x:x.split(\", \")[1])\n",
    "df_new = df_new.withColumn(\"city\", city_name(df_new.location))\n",
    "df_new = df_new.withColumn(\"state\", state_name(df_new.location))\n",
    "\n",
    "#split the useragent into browser and OS\n",
    "browser_name = udf(lambda x:user_agent_parser.ParseUserAgent(x)[\"family\"])\n",
    "os_name = udf(lambda x:user_agent_parser.ParseOS(x)[\"family\"])\n",
    "df_new = df_new.withColumn(\"browser\", browser_name(df_new.userAgent))\n",
    "df_new = df_new.withColumn(\"OS\", os_name(df_new.userAgent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis\n",
    "When you're working with the full dataset, perform EDA by loading a small subset of the data and doing basic manipulations within Spark. In this workspace, you are already provided a small subset of data you can explore.\n",
    "\n",
    "### Define Churn\n",
    "\n",
    "Once you've done some preliminary analysis, create a column `Churn` to use as the label for your model. I suggest using the `Cancellation Confirmation` events to define your churn, which happen for both paid and free users. As a bonus task, you can also look into the `Downgrade` events.\n",
    "\n",
    "### Explore Data\n",
    "Once you've defined churn, perform some exploratory data analysis to observe the behavior for users who stayed vs users who churned. You can start by exploring aggregates on these two groups of users, observing how much of a specific action they experienced per a certain time unit or number of songs played."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us explore some churn events. While we will add the downgrade indicator here, I will not explore it further, focusing on the Cancellation churn. Note the \"Cancel\" page column - this is very strongly correlated with the cancellation. This likely means very few back out after reaching this stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add flags for the two churn types, cancellations & downgrades. \n",
    "cancel_churn = udf(lambda x: int(x==\"Cancellation Confirmation\"), IntegerType())\n",
    "down_churn = udf(lambda x: int(x==\"Submit Downgrade\"), IntegerType())\n",
    "\n",
    "df_new = df_new.withColumn(\"Cancelled\", cancel_churn(\"page\"))\n",
    "df_new = df_new.withColumn(\"Downgrade\", down_churn(\"page\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.createOrReplaceTempView(\"spark_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---+\n",
      "|      state|cnt|\n",
      "+-----------+---+\n",
      "|         CA| 33|\n",
      "|         TX| 16|\n",
      "|   NY-NJ-PA| 15|\n",
      "|         FL| 14|\n",
      "|         AZ|  7|\n",
      "|         CT|  7|\n",
      "|      MO-IL|  6|\n",
      "|   IL-IN-WI|  6|\n",
      "|         NC|  6|\n",
      "|      NC-SC|  6|\n",
      "|      MA-NH|  5|\n",
      "|         NY|  5|\n",
      "|PA-NJ-DE-MD|  5|\n",
      "|         MI|  5|\n",
      "|         AL|  4|\n",
      "|DC-VA-MD-WV|  4|\n",
      "|         WA|  4|\n",
      "|         GA|  4|\n",
      "|         CO|  4|\n",
      "|      MN-WI|  3|\n",
      "+-----------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#explore a bit : Distribution by states\n",
    "state = spark.sql('''\n",
    "        SELECT state, COUNT(DISTINCT userId) AS cnt\n",
    "        FROM spark_table\n",
    "        GROUP BY State\n",
    "        ORDER BY cnt DESC\n",
    "        '''\n",
    "        ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most users are from California, evidently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+---+\n",
      "|Cancelled|gender|cnt|\n",
      "+---------+------+---+\n",
      "|        0|     F|104|\n",
      "|        0|     M|121|\n",
      "|        1|     F| 20|\n",
      "|        1|     M| 32|\n",
      "+---------+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# by Gender\n",
    "\n",
    "spark.sql('''\n",
    "        SELECT Cancelled, gender, COUNT(DISTINCT userId) AS cnt\n",
    "        FROM spark_table\n",
    "        GROUP BY Cancelled,gender \n",
    "        Order by Cancelled\n",
    "        '''\n",
    "        ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A higher number of male users, but the cancellations dont seem to be influenced by it, as the ratios are not too far off. Too few users to generalize here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------+---+\n",
      "|Cancelled|      Browser|cnt|\n",
      "+---------+-------------+---+\n",
      "|        0|           IE| 12|\n",
      "|        0|Mobile Safari| 16|\n",
      "|        0|       Safari| 30|\n",
      "|        0|      Firefox| 50|\n",
      "|        0|       Chrome|117|\n",
      "|        1|           IE|  1|\n",
      "|        1|Mobile Safari|  4|\n",
      "|        1|       Safari|  6|\n",
      "|        1|      Firefox| 16|\n",
      "|        1|       Chrome| 25|\n",
      "+---------+-------------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#by browser\n",
    "spark.sql('''\n",
    "        SELECT Cancelled, Browser, COUNT(DISTINCT userId) AS cnt\n",
    "        FROM spark_table\n",
    "        GROUP BY Cancelled, Browser\n",
    "        ORDER BY Cancelled\n",
    "        '''\n",
    "        ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+\n",
      "|      OS|cnt|\n",
      "+--------+---+\n",
      "| Windows|111|\n",
      "|Mac OS X| 86|\n",
      "|     iOS| 16|\n",
      "|   Linux|  7|\n",
      "|  Ubuntu|  5|\n",
      "+--------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#by platform\n",
    "spark.sql('''\n",
    "        SELECT OS, COUNT(DISTINCT userId) AS cnt\n",
    "        FROM spark_table\n",
    "        GROUP BY OS\n",
    "        ORDER BY cnt DESC\n",
    "        '''\n",
    "        ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "Once you've familiarized yourself with the data, build out the features you find promising to train your model on. To work with the full dataset, you can follow the following steps.\n",
    "- Write a script to extract the necessary features from the smaller subset of data\n",
    "- Ensure that your script is scalable, using the best practices discussed in Lesson 3\n",
    "- Try your script on the full data set, debugging your script if necessary\n",
    "\n",
    "If you are working in the classroom workspace, you can just extract features based on the small subset of data contained here. Be sure to transfer over this work to the larger dataset when you work on your Spark cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- artist: string (nullable = true)\n",
      " |-- auth: string (nullable = true)\n",
      " |-- firstName: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- itemInSession: long (nullable = true)\n",
      " |-- lastName: string (nullable = true)\n",
      " |-- length: double (nullable = true)\n",
      " |-- level: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- method: string (nullable = true)\n",
      " |-- page: string (nullable = true)\n",
      " |-- registration: long (nullable = true)\n",
      " |-- sessionId: long (nullable = true)\n",
      " |-- song: string (nullable = true)\n",
      " |-- status: long (nullable = true)\n",
      " |-- ts: long (nullable = true)\n",
      " |-- userAgent: string (nullable = true)\n",
      " |-- userId: string (nullable = true)\n",
      " |-- time: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- browser: string (nullable = true)\n",
      " |-- OS: string (nullable = true)\n",
      " |-- Cancelled: integer (nullable = true)\n",
      " |-- Downgrade: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_new.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical features :\n",
    "\n",
    "1.) Gender - A demographic variable.     \n",
    "2.) Level - Free vs paid - An essential variable\n",
    "3.) Browser - May indicate user demographics and technical acumen. Can also be used for mobile vs non mobile usage split\n",
    "4.) OS - Shows the technical acumen of the user, and can help target optimizations by platform\n",
    "5.) Cancelled : Predicting churn    \n",
    "\n",
    "For the future :  \n",
    "1.) State - A very sparse vector, may take the top 5 and put the rest to 0, as a demographic check (wealth etc) \n",
    "  \n",
    "### Numeric Features :  \n",
    "\n",
    "Time window based :  \n",
    "\n",
    "1.) Page events - Add friend, Add to Playlist etc. Skipping About, Settings, Save settings, Logout and the churn classifiers  - Different measures of activity\n",
    "\n",
    "2.) No. of songs, No. of artists, No. of sessions, listening time - To understand usage  \n",
    "\n",
    "Non time window based :  \n",
    "\n",
    "3.) How long the person has been a user for.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop unneeded columns\n",
    "\n",
    "df_selected = df_new.select(\"artist\", \"gender\", \"length\", \"level\", \"page\", \"registration\", \"sessionId\", \"song\", \"ts\", \"Time\", \"userId\", \"Browser\", \"OS\", \"Cancelled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix Time column - cast as date \n",
    "df_selected = df_selected.withColumn(\"Date\", df_selected[\"Time\"].cast(DateType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have an issue here - spark dataframes dont have an index. I can use ts (timestamp) as a proxy for an index, but two different users can have the same ts, which would make it non-unique. Hence, I will order by date and make an index column.\n",
    "\n",
    "This becomes important in my second approach at the problem (Approach 2: Event based)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add an index\n",
    "w = Window.orderBy(\"Date\")\n",
    "df_selected = df_selected.withColumn(\"index\", row_number().over(w))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[artist: string, gender: string, length: double, level: string, page: string, registration: bigint, sessionId: bigint, song: string, ts: bigint, Time: string, userId: string, Browser: string, OS: string, Cancelled: int, Date: date, index: int]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_selected.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Twin Approaches to the problem :\n",
    "\n",
    "When working on this, I realized that just 226 users are a bit too few for most machine learning models. Therefore, I decided on a dual approach :\n",
    "\n",
    "1.) Event Based : Use the churn event itself and model what led the user to it, vs other, non churn events. This of course, risks taking the events right preceding the churn events as a non-churn event. Likely will optimize this more in the future\n",
    "\n",
    "2.) User based : Check the churned user and see his pattern of usage vs the non-churned user. I will take the whole activity window for him, as the cancellation of the account (churn event) will remove him from our dataset. The limit : no. of users is just 226\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach 1 : Event based"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will first seperate the indexed events into two lists. This just made a number of exploratory approaches easier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use churn and not churn to find all indexes and make two lists\n",
    "churn = df_selected.filter(df_selected[\"Cancelled\"] == 1).select(\"index\").rdd.flatMap(lambda x: x).collect()\n",
    "not_churn = df_selected.filter(df_selected[\"Cancelled\"] == 0).select(\"index\").rdd.flatMap(lambda x: x).collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(churn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "278102"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(not_churn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, a huge number of not_churn events. We would need to take a sample here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#random sampling from not_churn, 1:10 ratio, fixing seed\n",
    "random.seed(50)\n",
    "not_churn = random.sample(not_churn, 520)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing any nulls\n",
    "df_selected = df_selected.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Creation function. Note that we output a list. This is because using a dataframe in an appending loop is a memory nightmare !\n",
    "\n",
    "def create_features_i(df_num, i):\n",
    "    '''\n",
    "    Create features for the dataframe df, given a specific index\n",
    "    \n",
    "    INPUT \n",
    "    df_num : Dataframe required\n",
    "    i : Relevant index\n",
    "    \n",
    "    OUTPUT\n",
    "    list_return : A list with the features\n",
    "    \n",
    "    '''\n",
    "\n",
    "    df_num.createOrReplaceTempView(\"num_table\")\n",
    "    \n",
    "    df_numerics = spark.sql('''\n",
    "                    SELECT\n",
    "                    MAX(index) AS index,\n",
    "                    COUNT(song) AS songs,\n",
    "                    COUNT(DISTINCT artist) AS artists,\n",
    "                    COUNT(DISTINCT sessionId) AS sessions,\n",
    "                    CAST(SUM(length) AS INT) AS list_time,\n",
    "                    CAST(((MAX(ts-registration))/10000) AS FLOAT) as age,\n",
    "                    SUM(CASE WHEN page = 'Add Friend' THEN 1 ELSE 0 END) AS frnd_cnt,\n",
    "                    SUM(CASE WHEN page = 'Roll Advert' THEN 1 ELSE 0 END) AS ad_cnt,\n",
    "                    SUM(CASE WHEN page = 'Add to Playlist' THEN 1 ELSE 0 END) AS playl_cnt,\n",
    "                    SUM(CASE WHEN page = 'Cancel' THEN 1 ELSE 0 END) AS cncl_cnt,\n",
    "                    SUM(CASE WHEN page = 'Error' THEN 1 ELSE 0 END) AS err_cnt,\n",
    "                    SUM(CASE WHEN page = 'Help' THEN 1 ELSE 0 END) AS hlp_cnt,\n",
    "                    SUM(CASE WHEN page = 'NextSong' THEN 1 ELSE 0 END) AS nxt_cnt,\n",
    "                    SUM(CASE WHEN page = 'Thumbs Down' THEN 1 ELSE 0 END) AS tbdn_cnt,\n",
    "                    SUM(CASE WHEN page = 'Thumbs Up' THEN 1 ELSE 0 END) AS tbup_cnt,\n",
    "                    SUM(CASE WHEN page = 'Upgrade' THEN 1 ELSE 0 END) AS up_cnt       \n",
    "                    FROM num_table\n",
    "                    GROUP BY userId\n",
    "          ''')            \n",
    "     \n",
    "    df_cat = df_num.filter(df_num[\"index\"]==i).select(\"index\", \"gender\", \"OS\", \"level\", \"browser\")\n",
    "    df_temp = df_numerics.join(df_cat, [\"index\"])\n",
    "    list_return = df_temp.toPandas().values.tolist()  \n",
    "    return list_return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, what we want to do is identify the churn event, look back at all events for that userid that led to it, and make it into a feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index 2479, no 12. \n",
      "Index 6121, no 13. \n",
      "Index 19215, no 14. \n",
      "Index 19437, no 15. \n",
      "Index 20089, no 16. \n",
      "Index 27964, no 17. \n",
      "Index 30928, no 18. \n",
      "Index 32593, no 19. \n",
      "Index 48117, no 20. \n",
      "Index 50855, no 21. \n",
      "Index 55498, no 22. \n",
      "Index 58751, no 23. \n",
      "Index 59979, no 24. \n",
      "Index 62458, no 25. \n",
      "Index 70431, no 26. \n",
      "Index 72170, no 27. \n",
      "Index 72615, no 28. \n",
      "Index 73867, no 29. \n",
      "Index 74003, no 30. \n",
      "Index 86102, no 31. \n",
      "Index 91578, no 32. \n",
      "Index 93622, no 33. \n",
      "Index 95929, no 34. \n",
      "Index 98701, no 35. \n",
      "Index 100100, no 36. \n",
      "Index 104008, no 37. \n",
      "Index 109834, no 38. \n",
      "Index 110133, no 39. \n",
      "Index 123459, no 40. \n",
      "Index 133868, no 41. \n",
      "Index 154296, no 42. \n",
      "Index 155523, no 43. \n",
      "Index 155944, no 44. \n",
      "Index 156093, no 45. \n",
      "Index 156226, no 46. \n",
      "Index 161896, no 47. \n",
      "Index 164171, no 48. \n",
      "Index 169517, no 49. \n",
      "Index 186354, no 50. \n",
      "Index 196552, no 51. \n",
      "Index 197154, no 52. \n",
      "Index 203639, no 53. \n",
      "Index 206866, no 54. \n",
      "Index 215979, no 55. \n",
      "Index 216381, no 56. \n",
      "Index 219643, no 57. \n",
      "Index 224484, no 58. \n",
      "Index 236701, no 59. \n",
      "Index 249196, no 60. \n",
      "Index 249367, no 61. \n",
      "Index 251727, no 62. \n",
      "Index 271640, no 63. \n"
     ]
    }
   ],
   "source": [
    "#build the churn dataset\n",
    "list_churn = []\n",
    "counter = 0\n",
    "for i in churn:\n",
    "    try:\n",
    "        user = df_selected.filter(df_selected[\"index\"]==i).select(\"userId\").head()[0]\n",
    "        end_date = df_selected.filter(df_selected[\"index\"]==i).select(\"Date\").head()[0]\n",
    "        d = datetime.timedelta(days = 30)\n",
    "        start_date = end_date - d\n",
    "        df_x = df_selected.filter((df_selected[\"Date\"].between(start_date, end_date)) & (df_selected[\"userId\"]==user) & (df_selected[\"index\"]<= i))\n",
    "        counter += 1\n",
    "        print(\"Index {}, no {}. \\r\".format(i, counter))\n",
    "        list_df = create_features_i(df_x, i)\n",
    "        list_churn.append(list_df)\n",
    "    except:\n",
    "        print(\"Error in index no. {}\".format(i))\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#quick fix for list of lists\n",
    "list_churn = [item for items in list_churn for item in items]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#writing the results in a pickle file\n",
    "with open('churn.pkl', 'wb') as f:\n",
    "    pickle.dump(list_churn, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index 260904, no 1. \n",
      "Index 139582, no 2. \n",
      "Index 190931, no 3. \n",
      "Index 127159, no 4. \n",
      "Index 248125, no 5. \n",
      "Index 173079, no 6. \n",
      "Index 44672, no 7. \n",
      "Index 166435, no 8. \n",
      "Index 117675, no 9. \n",
      "Index 44699, no 10. \n",
      "Index 80402, no 11. \n",
      "Index 182060, no 12. \n",
      "Index 51658, no 13. \n",
      "Index 182238, no 14. \n",
      "Index 167463, no 15. \n",
      "Index 116622, no 16. \n",
      "Index 99099, no 17. \n",
      "Index 36047, no 18. \n",
      "Index 172487, no 19. \n",
      "Index 224897, no 20. \n",
      "Index 46647, no 21. \n",
      "Index 33566, no 22. \n",
      "Index 175287, no 23. \n",
      "Index 225645, no 24. \n",
      "Index 3003, no 25. \n",
      "Index 119158, no 26. \n",
      "Index 176959, no 27. \n",
      "Index 139586, no 28. \n",
      "Index 227806, no 29. \n",
      "Index 55478, no 30. \n",
      "Index 105446, no 31. \n",
      "Index 232156, no 32. \n",
      "Index 142158, no 33. \n",
      "Index 241217, no 34. \n",
      "Index 66400, no 35. \n",
      "Index 141412, no 36. \n",
      "Index 109736, no 37. \n",
      "Index 64409, no 38. \n",
      "Index 215927, no 39. \n",
      "Index 61637, no 40. \n",
      "Index 260153, no 41. \n",
      "Index 223874, no 42. \n",
      "Index 271383, no 43. \n",
      "Index 65608, no 44. \n",
      "Index 261880, no 45. \n",
      "Index 270894, no 46. \n",
      "Index 157589, no 47. \n",
      "Index 48855, no 48. \n",
      "Index 192841, no 49. \n",
      "Index 270967, no 50. \n",
      "Index 78786, no 51. \n",
      "Index 3537, no 52. \n",
      "Index 187943, no 53. \n",
      "Index 4729, no 54. \n",
      "Index 215500, no 55. \n",
      "Index 247890, no 56. \n",
      "Index 117566, no 57. \n",
      "Index 85097, no 58. \n",
      "Index 51349, no 59. \n",
      "Index 71768, no 60. \n",
      "Index 134322, no 61. \n",
      "Index 197213, no 62. \n",
      "Index 128873, no 63. \n",
      "Index 176167, no 64. \n",
      "Index 17568, no 65. \n",
      "Index 271565, no 66. \n",
      "Index 24726, no 67. \n",
      "Index 41292, no 68. \n",
      "Index 124349, no 69. \n",
      "Index 137437, no 70. \n",
      "Index 277052, no 71. \n",
      "Index 220871, no 72. \n",
      "Index 243878, no 73. \n",
      "Index 219337, no 74. \n",
      "Index 204922, no 75. \n",
      "Index 246560, no 76. \n",
      "Index 46693, no 77. \n",
      "Index 211381, no 78. \n",
      "Index 63533, no 79. \n",
      "Index 190107, no 80. \n",
      "Index 27058, no 81. \n",
      "Index 10059, no 82. \n",
      "Index 35417, no 83. \n",
      "Index 232595, no 84. \n",
      "Index 4634, no 85. \n",
      "Index 161226, no 86. \n",
      "Index 166143, no 87. \n",
      "Index 181858, no 88. \n",
      "Index 272404, no 89. \n",
      "Index 261050, no 90. \n",
      "Index 46576, no 91. \n",
      "Index 158105, no 92. \n",
      "Index 83887, no 93. \n",
      "Index 225360, no 94. \n",
      "Index 102956, no 95. \n",
      "Index 58888, no 96. \n",
      "Index 46099, no 97. \n",
      "Index 5887, no 98. \n",
      "Index 83722, no 99. \n",
      "Index 69317, no 100. \n",
      "Index 26305, no 101. \n",
      "Index 75016, no 102. \n",
      "Index 15407, no 103. \n",
      "Index 163080, no 104. \n",
      "Index 186420, no 105. \n",
      "Index 8300, no 106. \n",
      "Index 93807, no 107. \n",
      "Index 262279, no 108. \n",
      "Index 27696, no 109. \n",
      "Index 126380, no 110. \n",
      "Index 161499, no 111. \n",
      "Index 13939, no 112. \n",
      "Index 78005, no 113. \n",
      "Index 259081, no 114. \n",
      "Index 211936, no 115. \n",
      "Index 226596, no 116. \n",
      "Index 182436, no 117. \n",
      "Index 80320, no 118. \n",
      "Index 117269, no 119. \n",
      "Index 14047, no 120. \n",
      "Index 273980, no 121. \n",
      "Index 148494, no 122. \n",
      "Index 175873, no 123. \n",
      "Index 198248, no 124. \n",
      "Index 170977, no 125. \n",
      "Index 23309, no 126. \n",
      "Index 76232, no 127. \n",
      "Index 251895, no 128. \n",
      "Index 149226, no 129. \n",
      "Index 66086, no 130. \n",
      "Index 59748, no 131. \n",
      "Index 165121, no 132. \n",
      "Index 198960, no 133. \n",
      "Index 133939, no 134. \n",
      "Index 269836, no 135. \n",
      "Index 64328, no 136. \n",
      "Index 52149, no 137. \n",
      "Index 35808, no 138. \n",
      "Index 14683, no 139. \n",
      "Index 55820, no 140. \n",
      "Index 157503, no 141. \n",
      "Index 254190, no 142. \n",
      "Index 244154, no 143. \n",
      "Index 131222, no 144. \n",
      "Index 154522, no 145. \n",
      "Index 130222, no 146. \n",
      "Index 138420, no 147. \n",
      "Index 117785, no 148. \n",
      "Index 104330, no 149. \n",
      "Index 130594, no 150. \n",
      "Index 156303, no 151. \n",
      "Index 129658, no 152. \n",
      "Index 25839, no 153. \n",
      "Index 261226, no 154. \n",
      "Index 96890, no 155. \n",
      "Index 126731, no 156. \n",
      "Index 60247, no 157. \n",
      "Index 144991, no 158. \n",
      "Index 199224, no 159. \n",
      "Index 837, no 160. \n",
      "Index 147843, no 161. \n",
      "Index 52637, no 162. \n",
      "Index 82483, no 163. \n",
      "Index 228706, no 164. \n",
      "Index 119480, no 165. \n",
      "Index 254531, no 166. \n",
      "Index 204964, no 167. \n",
      "Index 25576, no 168. \n",
      "Index 275224, no 169. \n",
      "Index 160596, no 170. \n",
      "Index 259372, no 171. \n",
      "Index 61655, no 172. \n",
      "Index 82477, no 173. \n",
      "Index 88003, no 174. \n",
      "Index 175520, no 175. \n",
      "Index 265927, no 176. \n",
      "Index 103337, no 177. \n",
      "Index 16050, no 178. \n",
      "Index 121160, no 179. \n",
      "Index 100830, no 180. \n",
      "Index 258140, no 181. \n",
      "Index 151347, no 182. \n",
      "Index 245700, no 183. \n",
      "Index 60591, no 184. \n",
      "Index 113397, no 185. \n",
      "Index 20067, no 186. \n",
      "Index 186692, no 187. \n",
      "Index 15774, no 188. \n",
      "Index 102386, no 189. \n",
      "Index 45822, no 190. \n",
      "Index 117706, no 191. \n",
      "Index 76455, no 192. \n",
      "Index 56265, no 193. \n",
      "Index 230784, no 194. \n",
      "Index 63060, no 195. \n",
      "Index 134944, no 196. \n",
      "Index 261346, no 197. \n",
      "Index 110890, no 198. \n",
      "Index 89684, no 199. \n",
      "Index 189311, no 200. \n",
      "Index 203426, no 201. \n",
      "Index 30057, no 202. \n",
      "Index 54128, no 203. \n",
      "Index 44231, no 204. \n",
      "Index 188886, no 205. \n",
      "Index 212957, no 206. \n",
      "Index 12543, no 207. \n",
      "Index 22578, no 208. \n",
      "Index 197969, no 209. \n",
      "Index 249754, no 210. \n",
      "Index 164112, no 211. \n",
      "Index 222001, no 212. \n",
      "Index 61576, no 213. \n",
      "Index 275622, no 214. \n",
      "Index 252105, no 215. \n",
      "Index 241819, no 216. \n",
      "Index 129207, no 217. \n",
      "Index 238790, no 218. \n",
      "Index 277698, no 219. \n",
      "Index 63473, no 220. \n",
      "Index 63713, no 221. \n",
      "Index 127369, no 222. \n",
      "Index 81706, no 223. \n",
      "Index 4597, no 224. \n",
      "Index 248396, no 225. \n",
      "Index 151977, no 226. \n",
      "Index 156939, no 227. \n",
      "Index 277635, no 228. \n",
      "Index 202800, no 229. \n",
      "Index 198113, no 230. \n",
      "Index 277032, no 231. \n",
      "Index 80328, no 232. \n",
      "Index 71920, no 233. \n",
      "Index 19818, no 234. \n",
      "Index 277727, no 235. \n",
      "Index 114034, no 236. \n",
      "Index 206522, no 237. \n",
      "Index 201364, no 238. \n",
      "Index 143419, no 239. \n",
      "Index 177030, no 240. \n",
      "Index 195595, no 241. \n",
      "Index 177537, no 242. \n",
      "Index 163032, no 243. \n",
      "Index 180907, no 244. \n",
      "Index 161188, no 245. \n",
      "Index 248669, no 246. \n",
      "Index 81108, no 247. \n",
      "Index 56260, no 248. \n",
      "Index 36998, no 249. \n",
      "Index 175691, no 250. \n",
      "Index 225755, no 251. \n",
      "Index 214044, no 252. \n",
      "Index 248460, no 253. \n",
      "Index 11569, no 254. \n",
      "Index 224380, no 255. \n",
      "Index 110471, no 256. \n",
      "Index 74585, no 257. \n",
      "Index 160256, no 258. \n",
      "Index 238359, no 259. \n",
      "Index 173994, no 260. \n",
      "Index 16543, no 261. \n",
      "Index 175020, no 262. \n",
      "Index 128753, no 263. \n",
      "Index 83301, no 264. \n",
      "Index 128551, no 265. \n",
      "Index 272870, no 266. \n",
      "Index 185199, no 267. \n",
      "Index 1044, no 268. \n",
      "Index 193896, no 269. \n",
      "Index 129825, no 270. \n",
      "Index 50987, no 271. \n",
      "Index 262381, no 272. \n",
      "Index 210475, no 273. \n",
      "Index 224479, no 274. \n",
      "Index 176141, no 275. \n",
      "Index 276383, no 276. \n",
      "Index 179495, no 277. \n",
      "Index 237455, no 278. \n",
      "Index 148199, no 279. \n",
      "Index 233962, no 280. \n",
      "Index 102081, no 281. \n",
      "Index 94081, no 282. \n",
      "Index 224783, no 283. \n",
      "Index 205447, no 284. \n",
      "Index 241346, no 285. \n",
      "Index 172408, no 286. \n",
      "Index 146024, no 287. \n",
      "Index 196239, no 288. \n",
      "Index 223750, no 289. \n",
      "Index 28716, no 290. \n",
      "Index 214877, no 291. \n",
      "Index 179920, no 292. \n",
      "Index 128268, no 293. \n",
      "Index 51903, no 294. \n",
      "Index 22507, no 295. \n",
      "Index 182870, no 296. \n",
      "Index 239266, no 297. \n",
      "Index 31213, no 298. \n",
      "Index 183563, no 299. \n",
      "Index 251743, no 300. \n",
      "Index 92404, no 301. \n",
      "Index 143316, no 302. \n",
      "Index 213190, no 303. \n",
      "Index 259832, no 304. \n",
      "Index 138182, no 305. \n",
      "Index 104514, no 306. \n",
      "Index 16023, no 307. \n",
      "Index 162161, no 308. \n",
      "Index 45830, no 309. \n",
      "Index 178837, no 310. \n",
      "Index 36910, no 311. \n",
      "Index 193272, no 312. \n",
      "Index 192542, no 313. \n",
      "Index 29657, no 314. \n",
      "Index 192489, no 315. \n",
      "Index 37727, no 316. \n",
      "Index 6947, no 317. \n",
      "Index 229749, no 318. \n",
      "Index 33578, no 319. \n",
      "Index 153212, no 320. \n",
      "Index 207371, no 321. \n",
      "Index 60904, no 322. \n",
      "Index 66924, no 323. \n",
      "Index 261022, no 324. \n",
      "Index 219564, no 325. \n",
      "Index 86319, no 326. \n",
      "Index 272800, no 327. \n",
      "Index 69712, no 328. \n",
      "Index 120661, no 329. \n",
      "Index 148714, no 330. \n",
      "Index 8848, no 331. \n",
      "Index 25076, no 332. \n",
      "Index 131344, no 333. \n",
      "Index 223983, no 334. \n",
      "Index 222677, no 335. \n",
      "Index 273292, no 336. \n",
      "Index 236377, no 337. \n",
      "Index 120857, no 338. \n",
      "Index 14526, no 339. \n",
      "Index 95326, no 340. \n",
      "Index 117570, no 341. \n",
      "Index 154250, no 342. \n",
      "Index 136590, no 343. \n",
      "Index 186459, no 344. \n",
      "Index 190295, no 345. \n",
      "Index 120097, no 346. \n",
      "Index 242739, no 347. \n",
      "Index 262111, no 348. \n",
      "Index 203706, no 349. \n",
      "Index 119108, no 350. \n",
      "Index 242569, no 351. \n",
      "Index 146539, no 352. \n",
      "Index 205607, no 353. \n",
      "Index 184313, no 354. \n",
      "Index 124828, no 355. \n",
      "Index 94847, no 356. \n",
      "Index 268691, no 357. \n",
      "Index 249382, no 358. \n",
      "Index 274649, no 359. \n",
      "Index 272919, no 360. \n",
      "Index 146936, no 361. \n",
      "Index 116446, no 362. \n",
      "Index 49236, no 363. \n",
      "Index 73847, no 364. \n",
      "Index 194256, no 365. \n",
      "Index 50425, no 366. \n",
      "Index 146911, no 367. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index 55811, no 368. \n",
      "Index 208164, no 369. \n",
      "Index 274291, no 370. \n",
      "Index 207127, no 371. \n",
      "Index 129928, no 372. \n",
      "Index 151184, no 373. \n",
      "Index 65305, no 374. \n",
      "Index 164472, no 375. \n",
      "Index 63013, no 376. \n",
      "Index 180664, no 377. \n",
      "Index 260303, no 378. \n",
      "Index 85363, no 379. \n",
      "Index 85430, no 380. \n",
      "Index 4815, no 381. \n",
      "Index 17987, no 382. \n",
      "Index 130905, no 383. \n",
      "Index 12575, no 384. \n",
      "Index 45057, no 385. \n",
      "Index 232719, no 386. \n",
      "Index 171861, no 387. \n",
      "Index 132118, no 388. \n",
      "Index 88346, no 389. \n",
      "Index 3158, no 390. \n",
      "Index 170127, no 391. \n",
      "Index 191982, no 392. \n",
      "Index 28719, no 393. \n",
      "Index 254863, no 394. \n",
      "Index 227210, no 395. \n",
      "Index 15304, no 396. \n",
      "Index 270994, no 397. \n",
      "Index 2596, no 398. \n",
      "Index 155731, no 399. \n",
      "Index 222114, no 400. \n",
      "Index 245139, no 401. \n",
      "Index 203783, no 402. \n",
      "Index 93385, no 403. \n",
      "Index 205896, no 404. \n",
      "Index 61815, no 405. \n",
      "Index 235935, no 406. \n",
      "Index 176805, no 407. \n",
      "Index 34947, no 408. \n",
      "Index 148850, no 409. \n",
      "Index 227013, no 410. \n",
      "Index 177671, no 411. \n",
      "Index 277101, no 412. \n",
      "Index 54800, no 413. \n",
      "Index 259824, no 414. \n",
      "Index 59343, no 415. \n",
      "Index 100107, no 416. \n",
      "Index 89916, no 417. \n",
      "Index 238794, no 418. \n",
      "Index 104837, no 419. \n",
      "Index 235981, no 420. \n",
      "Index 26068, no 421. \n",
      "Index 192956, no 422. \n",
      "Index 71446, no 423. \n",
      "Index 74656, no 424. \n",
      "Index 22109, no 425. \n",
      "Index 219843, no 426. \n",
      "Index 194237, no 427. \n",
      "Index 255987, no 428. \n",
      "Index 169650, no 429. \n",
      "Index 232995, no 430. \n",
      "Index 240442, no 431. \n",
      "Index 268506, no 432. \n",
      "Index 206478, no 433. \n",
      "Index 229666, no 434. \n",
      "Index 175519, no 435. \n",
      "Index 87705, no 436. \n",
      "Index 246129, no 437. \n",
      "Index 276352, no 438. \n",
      "Index 80364, no 439. \n",
      "Index 141365, no 440. \n",
      "Index 31143, no 441. \n",
      "Index 30599, no 442. \n",
      "Index 123311, no 443. \n",
      "Index 49319, no 444. \n",
      "Index 147707, no 445. \n",
      "Index 42620, no 446. \n",
      "Index 254523, no 447. \n",
      "Index 106161, no 448. \n",
      "Index 257341, no 449. \n",
      "Index 261851, no 450. \n",
      "Index 271243, no 451. \n",
      "Index 77445, no 452. \n",
      "Index 202269, no 453. \n",
      "Index 212301, no 454. \n",
      "Index 61962, no 455. \n",
      "Index 53723, no 456. \n",
      "Index 206239, no 457. \n",
      "Index 92366, no 458. \n",
      "Index 17291, no 459. \n",
      "Index 90438, no 460. \n",
      "Index 204387, no 461. \n",
      "Index 209561, no 462. \n",
      "Index 179739, no 463. \n",
      "Index 159635, no 464. \n",
      "Index 159552, no 465. \n",
      "Index 66579, no 466. \n",
      "Index 141561, no 467. \n",
      "Index 183372, no 468. \n",
      "Index 240575, no 469. \n",
      "Index 252616, no 470. \n",
      "Index 30112, no 471. \n",
      "Index 132403, no 472. \n",
      "Index 241533, no 473. \n",
      "Index 261736, no 474. \n",
      "Index 234421, no 475. \n",
      "Index 37033, no 476. \n",
      "Index 139567, no 477. \n",
      "Index 254399, no 478. \n",
      "Index 94133, no 479. \n",
      "Index 179204, no 480. \n",
      "Index 121571, no 481. \n",
      "Index 233946, no 482. \n",
      "Index 73175, no 483. \n",
      "Index 161306, no 484. \n",
      "Index 121810, no 485. \n",
      "Index 259458, no 486. \n",
      "Index 12190, no 487. \n",
      "Index 196307, no 488. \n",
      "Index 173454, no 489. \n",
      "Index 96190, no 490. \n",
      "Index 3172, no 491. \n",
      "Index 208938, no 492. \n",
      "Index 4602, no 493. \n",
      "Index 60940, no 494. \n",
      "Index 222222, no 495. \n",
      "Index 90966, no 496. \n",
      "Index 32446, no 497. \n",
      "Index 44406, no 498. \n",
      "Index 227196, no 499. \n",
      "Index 240239, no 500. \n",
      "Index 152131, no 501. \n",
      "Index 45237, no 502. \n",
      "Index 218302, no 503. \n",
      "Index 85876, no 504. \n",
      "Index 128018, no 505. \n",
      "Index 243693, no 506. \n",
      "Index 264174, no 507. \n",
      "Index 56172, no 508. \n",
      "Index 267587, no 509. \n",
      "Index 196025, no 510. \n",
      "Index 31965, no 511. \n",
      "Index 150936, no 512. \n",
      "Index 277509, no 513. \n",
      "Index 74314, no 514. \n",
      "Index 62159, no 515. \n",
      "Index 2576, no 516. \n",
      "Index 47613, no 517. \n",
      "Index 195624, no 518. \n",
      "Index 106664, no 519. \n",
      "Index 160885, no 520. \n"
     ]
    }
   ],
   "source": [
    "#build the not_churn dataset\n",
    "list_not_churn = []\n",
    "counter = 0\n",
    "\n",
    "for i in not_churn:\n",
    "    try:\n",
    "        user = df_selected.filter(df_selected[\"index\"]==i).select(\"userId\").head()[0]\n",
    "        end_date = df_selected.filter(df_selected[\"index\"]==i).select(\"Date\").head()[0]\n",
    "        d = datetime.timedelta(days = 30)\n",
    "        start_date = end_date - d\n",
    "        df_x = df_selected.filter((df_selected[\"Date\"].between(start_date, end_date)) & (df_selected[\"userId\"]==user) & (df_selected[\"index\"]<= i))\n",
    "        counter += 1\n",
    "        print(\"Index {}, no {}. \\r\".format(i, counter))\n",
    "        list_df = create_features_i(df_x, i)\n",
    "        list_not_churn.append(list_df)\n",
    "    except:\n",
    "        print(\"Error in index no. {}\".format(i))\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "#quick fix\n",
    "list_not_churn = [item for items in list_not_churn for item in items]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "#writing it into a file\n",
    "\n",
    "with open('not_churn.pkl', 'wb') as f:\n",
    "    pickle.dump(list_not_churn, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling - Approach 1\n",
    "Split the full dataset into train, test, and validation sets. Test out several of the machine learning methods you learned. Evaluate the accuracy of the various models, tuning parameters as necessary. Determine your winning model based on test accuracy and report results on the validation set. Since the churned users are a fairly small subset, I suggest using F1 score as the metric to optimize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create empty dataframe\n",
    "\n",
    "schema_1 = StructType([\n",
    "  StructField('index', IntegerType(), False),\n",
    "  StructField('songs', IntegerType(), False),\n",
    "  StructField('artists', IntegerType(), False),\n",
    "  StructField('sessions', IntegerType(), False),\n",
    "  StructField('list_time', IntegerType(), False),\n",
    "  StructField('age', FloatType(), False),\n",
    "  StructField('frnd_cnt', IntegerType(), False),\n",
    "  StructField('ad_cnt', IntegerType(), False),\n",
    "  StructField('playl_cnt', IntegerType(), False),\n",
    "  StructField('cncl_cnt', IntegerType(), False),\n",
    "  StructField('err_cnt', IntegerType(), False),\n",
    "  StructField('hlp_cnt', IntegerType(), False),\n",
    "  StructField('nxt_cnt', IntegerType(), False),\n",
    "  StructField('tbdn_cnt', IntegerType(), False),\n",
    "  StructField('tbup_cnt', IntegerType(), False),\n",
    "  StructField('up_cnt', IntegerType(), False),\n",
    "  StructField('gender', StringType(), False),\n",
    "  StructField('OS', StringType(), False),\n",
    "  StructField('level', StringType(), False),\n",
    "  StructField('browser', StringType(), False),\n",
    "  ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regenerating the churn list and converting it into a spark dataset\n",
    "\n",
    "with open('churn.pkl', 'rb') as f:\n",
    "    churn_list = pickle.load(f)\n",
    "    \n",
    "rdd = spark.sparkContext.parallelize(churn_list)\n",
    "df_churn = spark.createDataFrame(rdd,schema_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # regenerating the not_churn list and converting it into a spark dataset\n",
    "with open('not_churn.pkl', 'rb') as f:\n",
    "    not_churn_list = pickle.load(f)\n",
    "    \n",
    "rdd = spark.sparkContext.parallelize(not_churn_list)\n",
    "df_not_churn = spark.createDataFrame(rdd,schema_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add back the churn column as label variable\n",
    "df_churn = df_churn.withColumn(\"label\", lit(1))\n",
    "df_not_churn = df_not_churn.withColumn(\"label\", lit(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ready the final set of data\n",
    "df = df_churn.union(df_not_churn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Final indexing in prep for Encoding and vectorizing\n",
    "\n",
    "def indexing(df_cat, categories):\n",
    "    '''\n",
    "    Map Categorical labels to numbers \n",
    "    \n",
    "    INPUT \n",
    "    df_cat : Dataframe required\n",
    "    \n",
    "    OUTPUT\n",
    "    df_return : Dataframe with categorical features mapped to numbers\n",
    "    \n",
    "    '''\n",
    "\n",
    "    stages = []\n",
    "\n",
    "\n",
    "    for categoricalCol in categories:\n",
    "        stringIndexer = StringIndexer(inputCol = categoricalCol, outputCol = categoricalCol + '_idx')\n",
    "        stages += [stringIndexer]\n",
    "    \n",
    "    pipeline = Pipeline(stages = stages)\n",
    "    pipelineModel = pipeline.fit(df_cat)\n",
    "    df_return= pipelineModel.transform(df_cat)\n",
    "    \n",
    "    for categoricalCol in categories:\n",
    "        df_return = df_return.drop(categoricalCol)\n",
    "    \n",
    "    return df_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#map the categorical columns to numbers\n",
    "idx_col = [\"browser\", \"OS\", \"gender\", \"level\"]\n",
    "df = indexing(df, idx_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Browser and OS need one hot encoding\n",
    "encoder = OneHotEncoder(inputCols=[\"browser_idx\", \"OS_idx\"], outputCols=[\"browser_1\", \"OS_1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = encoder.fit(df)\n",
    "df = model.transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note : Here there is a key difference : I drop the \"Cancel\" feature column. The reason : It corresponds too closely to the event we want to measure. It of course means we lose data on users who do not press the confirm button - so maybe I will keep it for the bigger dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vector assembly\n",
    "vecAssembler = VectorAssembler(inputCols=['songs', 'artists','sessions', 'list_time', 'age', 'frnd_cnt', 'ad_cnt','playl_cnt','err_cnt','hlp_cnt','nxt_cnt','tbdn_cnt','tbup_cnt','up_cnt','browser_1','OS_1', 'gender_idx','level_idx' ], outputCol=\"features\")\n",
    "df = vecAssembler.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scaling\n",
    "scaler2 = MinMaxScaler(inputCol=\"features\", outputCol=\"ScaledNumFeatures2\")\n",
    "scalerModel = scaler2.fit(df)\n",
    "df = scalerModel.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split into test train. Due to the 1:10 ratio of churn to not churn, stratify it\n",
    "\n",
    "zeros = df.filter(df[\"label\"]==0)\n",
    "ones = df.filter(df[\"label\"]==1)\n",
    "# split datasets into training and testing\n",
    "\n",
    "train0, test0 = zeros.randomSplit([0.8,0.2], seed=50)\n",
    "train1, test1 = ones.randomSplit([0.8,0.2], seed=50)\n",
    "                                \n",
    "# stack datasets back together\n",
    "train = train0.union(train1)\n",
    "test = test0.union(test1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model : Logistic Regression\n",
    "\n",
    "One of the most widely used models, this model is fast, simple to use and works well with the limited data we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(featuresCol = 'ScaledNumFeatures2', labelCol = 'label', maxIter=5)\n",
    "lrmodel = lr.fit(train)\n",
    "predictions = lrmodel.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_evaluator = MulticlassClassificationEvaluator(labelCol = \"label\",predictionCol=\"prediction\")\n",
    "lr_accuracy = lr_evaluator.evaluate(predictions, {lr_evaluator.metricName: \"accuracy\"})\n",
    "lr_f1score = lr_evaluator.evaluate(predictions, {lr_evaluator.metricName: \"f1\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Metrics:\n",
      "Accuracy: 0.9482758620689655\n",
      "F-1 Score:0.9231003967043027\n",
      "Accuracy: 0.9482758620689655\n"
     ]
    }
   ],
   "source": [
    "print('Logistic Regression Metrics:')\n",
    "print('Accuracy: {}'.format(lr_accuracy))\n",
    "print('F-1 Score:{}'.format(lr_f1score))\n",
    "print('Accuracy: {}'.format(lr_accuracy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad ! The limited data really puts a question mark here, but the model seems to be doing well with an F1 score of 92%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model : Random Forest Classifier\n",
    "This model is a great model for small datasets, fast and deals well with features that may or may not be relevant to the final outcome at all. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"ScaledNumFeatures2\")\n",
    "rfmodel = rf.fit(train)\n",
    "predictions = rfmodel.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_evaluator = MulticlassClassificationEvaluator(labelCol = \"label\",predictionCol=\"prediction\")\n",
    "rf_accuracy = rf_evaluator.evaluate(predictions, {rf_evaluator.metricName: \"accuracy\"})\n",
    "rf_f1score = rf_evaluator.evaluate(predictions, {rf_evaluator.metricName: \"f1\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Randon Forest Metrics:\n",
      "Accuracy: 0.9482758620689655\n",
      "F-1 Score:0.9231003967043027\n"
     ]
    }
   ],
   "source": [
    "print('Randon Forest Metrics:')\n",
    "print('Accuracy: {}'.format(rf_accuracy))\n",
    "print('F-1 Score:{}'.format(rf_f1score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar Accuracy as regression. Lets try one more"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model : Gradient Boosted Trees\n",
    "A variation of Random forest that may work better here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbt = GBTClassifier(labelCol=\"label\", featuresCol=\"ScaledNumFeatures2\", maxIter=10)\n",
    "gbtmodel = gbt.fit(train)\n",
    "predictions = gbtmodel.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbt_evaluator = MulticlassClassificationEvaluator(labelCol = \"label\",predictionCol=\"prediction\")\n",
    "gbt_accuracy = gbt_evaluator.evaluate(predictions, {gbt_evaluator.metricName: \"accuracy\"})\n",
    "gbt_f1score = gbt_evaluator.evaluate(predictions, {gbt_evaluator.metricName: \"f1\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosted Trees Metrics:\n",
      "Accuracy: 0.9482758620689655\n",
      "F-1 Score:0.9231003967043027\n"
     ]
    }
   ],
   "source": [
    "print('Gradient Boosted Trees Metrics:')\n",
    "print('Accuracy: {}'.format(rf_accuracy))\n",
    "print('F-1 Score:{}'.format(rf_f1score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identical scores. I tried a few others but it seems we are hitting the limits of what we can predict with this approach here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach 2 : User based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use churn and not churn to find all userids and make two lists\n",
    "churn = df_selected.filter(df_selected[\"Cancelled\"] == 1).select(\"userId\").dropDuplicates().rdd.flatMap(lambda x: x).collect()\n",
    "not_churn = df_selected.filter(df_selected[\"Cancelled\"] == 0).select(\"userId\").dropDuplicates().rdd.flatMap(lambda x: x).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Creation function\n",
    "\n",
    "def create_features_u(df_num, i):\n",
    "    '''\n",
    "    Create features for the dataframe df, given a specific userid\n",
    "    \n",
    "    INPUT \n",
    "    df_num : Dataframe required\n",
    "    i : useriD\n",
    "    \n",
    "    OUTPUT\n",
    "    list_return : List with Features\n",
    "    \n",
    "    '''\n",
    "\n",
    "    df_num.createOrReplaceTempView(\"num_table\")\n",
    "    \n",
    "    df_numerics = spark.sql('''\n",
    "                    SELECT\n",
    "                    DISTINCT(userId),\n",
    "                    COUNT(song) AS songs,\n",
    "                    COUNT(DISTINCT artist) AS artists,\n",
    "                    COUNT(DISTINCT sessionId) AS sessions,\n",
    "                    CAST(SUM(length) AS INT) AS list_time,\n",
    "                    CAST(((MAX(ts-registration))/10000) AS FLOAT) as age,\n",
    "                    SUM(CASE WHEN page = 'Add Friend' THEN 1 ELSE 0 END) AS frnd_cnt,\n",
    "                    SUM(CASE WHEN page = 'Roll Advert' THEN 1 ELSE 0 END) AS ad_cnt,\n",
    "                    SUM(CASE WHEN page = 'Add to Playlist' THEN 1 ELSE 0 END) AS playl_cnt,\n",
    "                    SUM(CASE WHEN page = 'Cancel' THEN 1 ELSE 0 END) AS cncl_cnt,\n",
    "                    SUM(CASE WHEN page = 'Error' THEN 1 ELSE 0 END) AS err_cnt,\n",
    "                    SUM(CASE WHEN page = 'Help' THEN 1 ELSE 0 END) AS hlp_cnt,\n",
    "                    SUM(CASE WHEN page = 'NextSong' THEN 1 ELSE 0 END) AS nxt_cnt,\n",
    "                    SUM(CASE WHEN page = 'Thumbs Down' THEN 1 ELSE 0 END) AS tbdn_cnt,\n",
    "                    SUM(CASE WHEN page = 'Thumbs Up' THEN 1 ELSE 0 END) AS tbup_cnt,\n",
    "                    SUM(CASE WHEN page = 'Upgrade' THEN 1 ELSE 0 END) AS up_cnt       \n",
    "                    FROM num_table\n",
    "                    GROUP BY userId\n",
    "          ''')            \n",
    "    # Lets join in the categorical features. We use the last known status based on our index\n",
    "    \n",
    "    df_cat = df_num.filter(df_num[\"userId\"]==i).orderBy(\"index\", ascending=False).limit(1).select(\"userId\", \"gender\", \"OS\", \"level\", \"browser\")\n",
    "    df_temp = df_numerics.join(df_cat, [\"userId\"])\n",
    "    list_return = df_temp.toPandas().values.tolist()  \n",
    "    return list_return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build the churn dataset\n",
    "list_churn = []\n",
    "counter = 0\n",
    "for i in churn:\n",
    "    try:\n",
    "        user = i\n",
    "        counter += 1\n",
    "        print(\"Index {}, no {}. \\r\".format(i, counter))\n",
    "        list_df = create_features_u(df_selected, i)\n",
    "        list_churn.append(list_df)\n",
    "    except:\n",
    "        print(\"Error in index no. {}\".format(i))\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build the not_churn dataset\n",
    "list_not_churn = []\n",
    "counter = 0\n",
    "\n",
    "for i in not_churn:\n",
    "    try:\n",
    "        user = i\n",
    "        counter += 1\n",
    "        print(\"Index {}, no {}. \\r\".format(i, counter))\n",
    "        list_df = create_features_u(df_selected, i)\n",
    "        list_not_churn.append(list_df)\n",
    "    except:\n",
    "        print(\"Error in index no. {}\".format(i))\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fixes \n",
    "list_churn = [item for items in list_churn for item in items]\n",
    "list_not_churn = [item for items in list_not_churn for item in items]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write to Pickle\n",
    "with open('churn2.pkl', 'wb') as f:\n",
    "    pickle.dump(list_churn, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('not_churn2.pkl', 'wb') as f:\n",
    "    pickle.dump(list_not_churn, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling - Approach 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create empty dataframe for vectors\n",
    "\n",
    "schema_2 = StructType([\n",
    "  StructField('userId', StringType(), False),\n",
    "  StructField('songs', IntegerType(), False),\n",
    "  StructField('artists', IntegerType(), False),\n",
    "  StructField('sessions', IntegerType(), False),\n",
    "  StructField('list_time', IntegerType(), False),\n",
    "  StructField('age', FloatType(), False),\n",
    "  StructField('frnd_cnt', IntegerType(), False),\n",
    "  StructField('ad_cnt', IntegerType(), False),\n",
    "  StructField('playl_cnt', IntegerType(), False),\n",
    "  StructField('cncl_cnt', IntegerType(), False),\n",
    "  StructField('err_cnt', IntegerType(), False),\n",
    "  StructField('hlp_cnt', IntegerType(), False),\n",
    "  StructField('nxt_cnt', IntegerType(), False),\n",
    "  StructField('tbdn_cnt', IntegerType(), False),\n",
    "  StructField('tbup_cnt', IntegerType(), False),\n",
    "  StructField('up_cnt', IntegerType(), False),\n",
    "  StructField('gender', StringType(), False),\n",
    "  StructField('OS', StringType(), False),\n",
    "  StructField('level', StringType(), False),\n",
    "  StructField('browser', StringType(), False),\n",
    "  ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in the lists from pickle and create dataframe\n",
    "with open('churn2.pkl', 'rb') as f:\n",
    "    churn_list = pickle.load(f)\n",
    "    \n",
    "rdd = spark.sparkContext.parallelize(churn_list)\n",
    "df_churn = spark.createDataFrame(rdd,schema_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('not_churn2.pkl', 'rb') as f:\n",
    "    not_churn_list = pickle.load(f)\n",
    "    \n",
    "rdd = spark.sparkContext.parallelize(not_churn_list)\n",
    "df_not_churn = spark.createDataFrame(rdd,schema_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add back the churn column as label variable\n",
    "df_churn = df_churn.withColumn(\"label\", lit(1))\n",
    "df_not_churn = df_not_churn.withColumn(\"label\", lit(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ready the final set\n",
    "\n",
    "df = df_churn.union(df_not_churn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Final indexing in prep for Encoding and vectorizing\n",
    "\n",
    "def indexing(df_cat, categories):\n",
    "    '''\n",
    "    Map Categorical labels to numbers \n",
    "    \n",
    "    INPUT \n",
    "    df_cat : Dataframe required\n",
    "    \n",
    "    OUTPUT\n",
    "    df_return : Dataframe with categorical features mapped to numbers\n",
    "    \n",
    "    '''\n",
    "\n",
    "    stages = []\n",
    "\n",
    "\n",
    "    for categoricalCol in categories:\n",
    "        stringIndexer = StringIndexer(inputCol = categoricalCol, outputCol = categoricalCol + '_idx')\n",
    "        stages += [stringIndexer]\n",
    "    \n",
    "    pipeline = Pipeline(stages = stages)\n",
    "    pipelineModel = pipeline.fit(df_cat)\n",
    "    df_return= pipelineModel.transform(df_cat)\n",
    "    \n",
    "    for categoricalCol in categories:\n",
    "        df_return = df_return.drop(categoricalCol)\n",
    "    \n",
    "    return df_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#map the categorical columns to numbers\n",
    "idx_col = [\"browser\", \"OS\", \"gender\", \"level\"]\n",
    "df = indexing(df, idx_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Browser and OS need one hot encoding\n",
    "encoder = OneHotEncoder(inputCols=[\"browser_idx\", \"OS_idx\"], outputCols=[\"browser_1\", \"OS_1\"])\n",
    "model = encoder.fit(df)\n",
    "df = model.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vector assembly\n",
    "vecAssembler = VectorAssembler(inputCols=['songs', 'artists','sessions', 'list_time', 'age', 'frnd_cnt', 'ad_cnt','playl_cnt','err_cnt','hlp_cnt','nxt_cnt','tbdn_cnt','tbup_cnt','up_cnt','browser_1','OS_1', 'gender_idx','level_idx' ], outputCol=\"features\")\n",
    "df = vecAssembler.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scaling\n",
    "scaler2 = MinMaxScaler(inputCol=\"features\", outputCol=\"ScaledNumFeatures2\")\n",
    "scalerModel = scaler2.fit(df)\n",
    "df = scalerModel.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split into test train. Due to the low ration of churn events, stratifying it\n",
    "\n",
    "zeros = df.filter(df[\"label\"]==0)\n",
    "ones = df.filter(df[\"label\"]==1)\n",
    "# split datasets into training and testing\n",
    "\n",
    "train0, test0 = zeros.randomSplit([0.8,0.2], seed=50)\n",
    "train1, test1 = ones.randomSplit([0.8,0.2], seed=50)\n",
    "                                \n",
    "# stack datasets back together\n",
    "train = train0.union(train1)\n",
    "test = test0.union(test1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model : Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting and training\n",
    "lr = LogisticRegression(featuresCol = 'ScaledNumFeatures2', labelCol = 'label', maxIter=5)\n",
    "lrmodel = lr.fit(train)\n",
    "predictions = lrmodel.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluating\n",
    "lr_evaluator = MulticlassClassificationEvaluator(labelCol = \"label\",predictionCol=\"prediction\")\n",
    "lr_accuracy = lr_evaluator.evaluate(predictions, {lr_evaluator.metricName: \"accuracy\"})\n",
    "lr_f1score = lr_evaluator.evaluate(predictions, {lr_evaluator.metricName: \"f1\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Metrics:\n",
      "Accuracy: 0.896551724137931\n",
      "F-1 Score:0.8476489028213167\n"
     ]
    }
   ],
   "source": [
    "print('Logistic Regression Metrics:')\n",
    "print('Accuracy: {}'.format(lr_accuracy))\n",
    "print('F-1 Score:{}'.format(lr_f1score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A decent F1 score of 84%, though lower than approach 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model : Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting and training\n",
    "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"ScaledNumFeatures2\")\n",
    "rfmodel = rf.fit(train)\n",
    "predictions = rfmodel.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluating\n",
    "rf_evaluator = MulticlassClassificationEvaluator(labelCol = \"label\",predictionCol=\"prediction\")\n",
    "rf_accuracy = rf_evaluator.evaluate(predictions, {rf_evaluator.metricName: \"accuracy\"})\n",
    "rf_f1score = rf_evaluator.evaluate(predictions, {rf_evaluator.metricName: \"f1\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Randon Forest Metrics:\n",
      "Accuracy: 0.7758620689655172\n",
      "F-1 Score:0.7833947104117844\n"
     ]
    }
   ],
   "source": [
    "print('Randon Forest Metrics:')\n",
    "print('Accuracy: {}'.format(rf_accuracy))\n",
    "print('F-1 Score:{}'.format(rf_f1score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lower f1 score, which is interesting, as I expected this to perform better - I assumed a lot of variables we took were noise and not really explanatory. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model : Gradient Boosted Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting and training\n",
    "gbt = GBTClassifier(labelCol=\"label\", featuresCol=\"ScaledNumFeatures2\", maxIter=10)\n",
    "gbtmodel = gbt.fit(train)\n",
    "predictions = gbtmodel.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluating\n",
    "gbt_evaluator = MulticlassClassificationEvaluator(labelCol = \"label\",predictionCol=\"prediction\")\n",
    "gbt_accuracy = gbt_evaluator.evaluate(predictions, {gbt_evaluator.metricName: \"accuracy\"})\n",
    "gbt_f1score = gbt_evaluator.evaluate(predictions, {gbt_evaluator.metricName: \"f1\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosted Trees Metrics:\n",
      "Accuracy: 0.7758620689655172\n",
      "F-1 Score:0.7833947104117844\n"
     ]
    }
   ],
   "source": [
    "print('Gradient Boosted Trees Metrics:')\n",
    "print('Accuracy: {}'.format(rf_accuracy))\n",
    "print('F-1 Score:{}'.format(rf_f1score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No change from the Random forest classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion:\n",
    "\n",
    "It seems the first approach has a better accuracy, but the second likely makes use of the demographic variables better, and thus may perform better in the real world. The logistic regression model performs better on most fronts here. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Steps\n",
    "Clean up your code, adding comments and renaming variables to make the code easier to read and maintain. Refer to the Spark Project Overview page and Data Scientist Capstone Project Rubric to make sure you are including all components of the capstone project and meet all expectations. Remember, this includes thorough documentation in a README file in a Github repository, as well as a web app or blog post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
